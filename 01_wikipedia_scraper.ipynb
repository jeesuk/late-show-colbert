{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OVERVIEW\n",
    "\n",
    "Prior to the inauguration of President Trump on January 20th, 2017, 'The Late Show' struggled to find its voice with Stephen Colbert as its host. As a venerable late night institution formerly helmed by David Letterman, its mandate was typical of a broadcast late night talk show -- to appeal to a broad audience with universally likable guests and humor that doesn't alienate viewers. Colbert, however, was known by his loyal base for his liberal-skewing humor on 'The Daily Show' and 'The Colbert Report.' Meanwhile, at NBC, Jimmy Fallon employed his brand of inoffensive humor to great effect on 'The Tonight Show,' and held the lead in late night talk ratings until Trump's inauguration.\n",
    "\n",
    "After Trump's inauguration, however, Colbert embraced his politically charged humor. All of his monologues since have featured criticisms of Trump, Trump's advisors, GOP congressmen, and other GOP figures. He has interviewed more guests within the realm of politics, including officials, journalists, and fellow comedians. Fallon, opting not to take a stance on politically divisive issues, has seen his ratings decline and lose share to Colbert. This past season, running from September 2018 to May 2019, Colbert usurped Fallon as the king of late night. Rumors of turnover and turmoil among 'The Tonight Show' staff surfaced in May, as NBC scrambles to regain its footing.\n",
    "\n",
    "As political content has never been a mainstay of a traditional late night talk show, this analysis will explore the significance of Colbert's interviews with politics-adjacent guests by looking at their YouTube views data, and whether these interviews attract a substantial amount of the show's total views."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LATE SHOW WIKIPEDIA SCRAPE\n",
    "\n",
    "Using a Wikipedia scraper from https://roche.io/2016/05/scrape-wikipedia-with-python. Modified for the 'Late Show' Wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import platform\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import shutil\n",
    "\n",
    "def scrape(url, output_name):\n",
    "    \"\"\"Create CSVs from all tables in a Wikipedia article.\n",
    "    ARGS:\n",
    "        url (str): The full URL of the Wikipedia article to scrape tables from.\n",
    "        output_name (str): The base file name (without filepath) to write to.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read tables from Wikipedia article into list of HTML strings\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.content, 'lxml')\n",
    "    table_classes = {\"class\": [\"sortable\", \"plainrowheaders\"]}\n",
    "    wikitables = soup.findAll(\"table\", table_classes)\n",
    "\n",
    "    # Create folder for output if it doesn't exist\n",
    "#     try:\n",
    "#         os.mkdir(output_name)\n",
    "#     except Exception:  # Generic OS Error\n",
    "#         pass\n",
    "\n",
    "    for index, table in enumerate(wikitables):\n",
    "        # Make a unique file name for each CSV\n",
    "        filename = output_name + '_' + str(index)\n",
    "\n",
    "        filepath = os.path.join('../data/', filename) + '.csv'\n",
    "\n",
    "        with open(filepath, mode='w', newline='', encoding='utf-8') as output:\n",
    "            # Deal with Windows inserting an extra '\\r' in line terminators\n",
    "            if platform.system() == 'Windows':\n",
    "                kwargs = {'lineterminator': '\\n'}\n",
    "\n",
    "                csv_writer = csv.writer(output,\n",
    "                                        quoting=csv.QUOTE_ALL,\n",
    "                                        **kwargs)\n",
    "            else:\n",
    "                csv_writer = csv.writer(output,\n",
    "                                        quoting=csv.QUOTE_ALL)\n",
    "\n",
    "            write_html_table_to_csv(table, csv_writer)\n",
    "\n",
    "\n",
    "def write_html_table_to_csv(table, writer):\n",
    "    \"\"\"Write HTML table from Wikipedia to a CSV file.\n",
    "    ARGS:\n",
    "        table (bs4.Tag): The bs4 Tag object being analyzed.\n",
    "        writer (csv.writer): The csv Writer object creating the output.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hold elements that span multiple rows in a list of\n",
    "    # dictionaries that track 'rows_left' and 'value'\n",
    "    saved_rowspans = []\n",
    "    for row in table.findAll(\"tr\"):\n",
    "        cells = row.findAll([\"th\", \"td\"])\n",
    "\n",
    "        # If the first row, use it to define width of table\n",
    "        if len(saved_rowspans) == 0:\n",
    "            saved_rowspans = [None for _ in cells]\n",
    "        # Insert values from cells that span into this row\n",
    "        elif len(cells) != len(saved_rowspans):\n",
    "            for index, rowspan_data in enumerate(saved_rowspans):\n",
    "                if rowspan_data is not None:\n",
    "                    # Insert the data from previous row; decrement rows left\n",
    "                    value = rowspan_data['value']\n",
    "                    cells.insert(index, value)\n",
    "\n",
    "                    if saved_rowspans[index]['rows_left'] == 1:\n",
    "                        saved_rowspans[index] = None\n",
    "                    else:\n",
    "                        saved_rowspans[index]['rows_left'] -= 1\n",
    "\n",
    "        # If an element with rowspan, save it for future cells\n",
    "        for index, cell in enumerate(cells):\n",
    "            if cell.has_attr(\"rowspan\"):\n",
    "                rowspan_data = {\n",
    "                    'rows_left': int(cell[\"rowspan\"]),\n",
    "                    'value': cell,\n",
    "                }\n",
    "                saved_rowspans[index] = rowspan_data\n",
    "\n",
    "        if cells:\n",
    "            # Clean the data of references and unusual whitespace\n",
    "            cleaned = clean_data(cells)\n",
    "\n",
    "            # Fill the row with empty columns if some are missing\n",
    "            # (Some HTML tables leave final empty cells without a <td> tag)\n",
    "            columns_missing = len(saved_rowspans) - len(cleaned)\n",
    "            if columns_missing:\n",
    "                cleaned += [None] * columns_missing\n",
    "\n",
    "            writer.writerow(cleaned)\n",
    "\n",
    "\n",
    "def clean_data(row):\n",
    "    \"\"\"Clean table row list from Wikipedia into a string for CSV.\n",
    "    ARGS:\n",
    "        row (bs4.ResultSet): The bs4 result set being cleaned for output.\n",
    "    RETURNS:\n",
    "        cleaned_cells (list[str]): List of cleaned text items in this row.\n",
    "    \"\"\"\n",
    "\n",
    "    cleaned_cells = []\n",
    "\n",
    "    for cell in row:\n",
    "        # Strip references from the cell\n",
    "        references = cell.findAll(\"sup\", {\"class\": \"reference\"})\n",
    "        if references:\n",
    "            for ref in references:\n",
    "                ref.extract()\n",
    "\n",
    "        # Strip sortkeys from the cell\n",
    "        sortkeys = cell.findAll(\"span\", {\"class\": \"sortkey\"})\n",
    "        if sortkeys:\n",
    "            for ref in sortkeys:\n",
    "                ref.extract()\n",
    "\n",
    "        # Strip footnotes from text and join into a single string\n",
    "        text_items = cell.findAll(text=True)\n",
    "        no_footnotes = [text for text in text_items if text[0] != '[']\n",
    "\n",
    "        cleaned = (\n",
    "            ''.join(no_footnotes)  # Combine elements into single string\n",
    "            .replace('\\xa0', ' ')  # Replace non-breaking spaces\n",
    "            .replace('\\n', ' ')  # Replace newlines\n",
    "            .strip()\n",
    "        )\n",
    "\n",
    "        cleaned_cells += [cleaned]\n",
    "\n",
    "    return cleaned_cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assembling the CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape(\n",
    "    url=\"https://en.wikipedia.org/wiki/List_of_The_Late_Show_with_Stephen_Colbert_episodes_(2015)\",\n",
    "    output_name=\"LSSC_2015\")\n",
    "\n",
    "scrape(\n",
    "    url=\"https://en.wikipedia.org/wiki/List_of_The_Late_Show_with_Stephen_Colbert_episodes_(2016)\",\n",
    "    output_name=\"LSSC_2016\")\n",
    "\n",
    "scrape(\n",
    "    url=\"https://en.wikipedia.org/wiki/List_of_The_Late_Show_with_Stephen_Colbert_episodes_(2017)\",\n",
    "    output_name=\"LSSC_2017\")\n",
    "\n",
    "scrape(\n",
    "    url=\"https://en.wikipedia.org/wiki/List_of_The_Late_Show_with_Stephen_Colbert_episodes_(2018)\",\n",
    "    output_name=\"LSSC_2018\")\n",
    "\n",
    "scrape(\n",
    "    url=\"https://en.wikipedia.org/wiki/List_of_The_Late_Show_with_Stephen_Colbert_episodes_(2019)\",\n",
    "    output_name=\"LSSC_2019\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
